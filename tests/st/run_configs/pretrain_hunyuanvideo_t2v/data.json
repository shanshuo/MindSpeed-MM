{
    "dataset_param": {
        "dataset_type": "feature",
        "basic_parameters": {
            "data_path": "/home/ci_resource/data/hunyuanvideo_t2v/feature_data/data.jsonl",
            "data_folder": "/home/ci_resource/data/hunyuanvideo_t2v/feature_data/"
        },
        "preprocess_parameters": {
            "video_processor_type": "OpensoraplanVideoProcessor",
            "video_reader_type": "DecordVideo",
            "image_reader_type": "Image",
            "num_frames": 93,
            "frame_interval": 1,
            "max_height": 480,
            "max_width": 848,
            "max_hxw": 407040,
            "train_fps": 24,
            "speed_factor": 1.0,
            "drop_short_ratio": 0.0,
            "cfg": 0.0,
            "hw_stride": 8,
            "force_resolution": false,
            "hw_aspect_thr": 2.0,
            "train_pipeline": {
                "video": [
                    {
                        "trans_type": "ToTensorVideo"
                    },
                    {
                        "trans_type": "CenterCropResizeVideo",
                        "param": {
                            "use_short_edge": false,
							"top_crop": false,
							"align_corners": false,
							"antialias": false,
							"transform_size": "auto"
                        }
                    },
                    {
                        "trans_type": "norm_fun",
                        "param": {
                            "mean": 0.5,
                            "std": 0.5
                        }
                    }
                ]
            }
        },
        "use_text_processer": true,
        "enable_text_preprocessing": false,
        "tokenizer_config": [
            {
                "autotokenizer_name": "AutoTokenizer",
                "hub_backend": "hf",
                "from_pretrained": "/home/ci_resource/models/text_encoder_mini/llava-llama-3-8b-v1_1-transformers_mini/",
                "model_max_length": 256
            },
            {
                "autotokenizer_name": "CLIPTokenizer",
                "hub_backend": "hf",
                "from_pretrained": "/home/ci_resource/models/clip-vit-large-patch14/",
                "model_max_length": 77
            }
        ]
    },
    "dataloader_param": {
        "dataloader_mode": "sampler",
        "sampler_type": "LengthGroupedSampler",
        "shuffle": true,
        "drop_last": true,
        "pin_memory": true,
        "group_frame": false,
        "group_resolution": false,
        "group_data": true,
        "initial_global_step_for_sampler": 0
    }
}